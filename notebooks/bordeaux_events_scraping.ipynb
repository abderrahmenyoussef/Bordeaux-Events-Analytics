{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88e0f97",
   "metadata": {},
   "source": [
    "# Collecte des Donn√©es d'√âv√©nements √† Bordeaux \n",
    "\n",
    "## Contexte du Projet\n",
    "La ville de Bordeaux organise de nombreux √©v√©nements (culturels, sportifs, √©conomiques, etc.) chaque mois. L'objectif de ce notebook est de collecter, traiter et analyser les donn√©es non structur√©es issues du web pour les √©v√©nements pr√©vus en septembre 2025.\n",
    "\n",
    "## Objectifs\n",
    "1. **Identifier** les sources d'√©v√©nements en ligne\n",
    "2. **Scraper** les √©v√©nements pr√©vus √† Bordeaux pour septembre 2025\n",
    "3. **Nettoyer** et structurer les donn√©es collect√©es\n",
    "4. **Sauvegarder** les donn√©es pour analyse ult√©rieure\n",
    "\n",
    "## Sources Cibl√©es\n",
    "- Sites officiels de la ville de Bordeaux\n",
    "- Agendas culturels (th√©√¢tres, mus√©es, salles de concert)\n",
    "- Plateformes sp√©cialis√©es en √©v√©nements\n",
    "- Sites de venues sportives et √©conomiques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903092a6",
   "metadata": {},
   "source": [
    "## 1. Import des Biblioth√®ques N√©cessaires\n",
    "\n",
    "Nous importons toutes les biblioth√®ques n√©cessaires pour le web scraping, le traitement des donn√©es et la gestion des d√©lais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9563ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Toutes les biblioth√®ques ont √©t√© import√©es avec succ√®s\n"
     ]
    }
   ],
   "source": [
    "# Biblioth√®ques pour le web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Biblioth√®ques pour le traitement des donn√©es\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import dateparser\n",
    "\n",
    "# Biblioth√®ques utilitaires\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Toutes les biblioth√®ques ont √©t√© import√©es avec succ√®s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68305e",
   "metadata": {},
   "source": [
    "## 2. Configuration des Outils de Web Scraping\n",
    "\n",
    "Configuration des headers HTTP, des d√©lais entre les requ√™tes et du driver Selenium pour les sites dynamiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f183c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration des outils de scraping termin√©e\n"
     ]
    }
   ],
   "source": [
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Headers pour les requ√™tes HTTP (simuler un navigateur)\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "# Configuration de Selenium (pour les sites avec JavaScript)\n",
    "def setup_selenium_driver():\n",
    "    \"\"\"Configure et retourne un driver Selenium Chrome en mode headless\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument(f'user-agent={HEADERS[\"User-Agent\"]}')\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Impossible de configurer Selenium: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour g√©rer les d√©lais entre les requ√™tes\n",
    "def random_delay(min_seconds=1, max_seconds=3):\n",
    "    \"\"\"Ajoute un d√©lai al√©atoire pour √©viter la d√©tection de bot\"\"\"\n",
    "    delay = random.uniform(min_seconds, max_seconds)\n",
    "    time.sleep(delay)\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration des outils de scraping termin√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafdd68",
   "metadata": {},
   "source": [
    "## 3. D√©finition des Sites Web et URLs Cibles\n",
    "\n",
    "Liste des sources principales pour collecter les √©v√©nements bordelais de septembre 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2290fa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Sources optimis√©es par cat√©gorie:\n",
      "\n",
      "üèõÔ∏è Sources officielles (2):\n",
      "  - Bordeaux M√©tropole\n",
      "  - Ville de Bordeaux\n",
      "\n",
      "üé≠ Sources culturelles (4):\n",
      "  - Op√©ra National de Bordeaux\n",
      "  - Cap Sciences Bordeaux\n",
      "  - Bassins des Lumi√®res\n",
      "  - Mus√©e d'Aquitaine\n",
      "\n",
      "‚öΩ Sources sportives (3):\n",
      "  - Union Bordeaux-B√®gles Rugby\n",
      "  - Stade de Bordeaux - Matmut Atlantique\n",
      "  - Bordeaux M√©tropole Sports\n",
      "\n",
      "üíº Sources √©conomiques (1):\n",
      "  - Bordeaux Business Events\n",
      "\n",
      "üåê Plateformes sp√©cialis√©es (2):\n",
      "  - Universit√© de Bordeaux\n",
      "  - Bordeaux M√©tropole - Agenda Complet\n",
      "\n",
      "üìä Total: 12 sources optimis√©es\n"
     ]
    }
   ],
   "source": [
    "# Sites officiels de Bordeaux - Sources valid√©es et productives\n",
    "OFFICIAL_SOURCES = {\n",
    "    'bordeaux_metropole': {\n",
    "        'name': 'Bordeaux M√©tropole',\n",
    "        'url': 'https://www.bordeaux-metropole.fr/agenda',\n",
    "        'type': 'official'\n",
    "    },\n",
    "    'bordeaux_ville': {\n",
    "        'name': 'Ville de Bordeaux',\n",
    "        'url': 'https://www.bordeaux.fr/evenements',\n",
    "        'type': 'official'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plateformes culturelles - URLs corrig√©es\n",
    "CULTURAL_SOURCES = {\n",
    "    'opera_bordeaux': {\n",
    "        'name': 'Op√©ra National de Bordeaux',\n",
    "        'url': 'https://www.opera-bordeaux.com/fr/saison',\n",
    "        'type': 'cultural'\n",
    "    },\n",
    "    'cap_sciences': {\n",
    "        'name': 'Cap Sciences Bordeaux',\n",
    "        'url': 'https://www.cap-sciences.net/',  # URL corrig√©e\n",
    "        'type': 'cultural'\n",
    "    },\n",
    "    'bassins_lumieres': {\n",
    "        'name': 'Bassins des Lumi√®res',\n",
    "        'url': 'https://www.bassins-lumieres.com/fr/home',\n",
    "        'type': 'cultural'\n",
    "    },\n",
    "    'musee_aquitaine': {\n",
    "        'name': 'Mus√©e d\\'Aquitaine',\n",
    "        'url': 'https://www.musee-aquitaine-bordeaux.fr/fr/agenda',  # Source alternative\n",
    "        'type': 'cultural'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sources sportives - URLs mises √† jour et test√©es\n",
    "SPORTS_SOURCES = {\n",
    "    'ubb_rugby': {\n",
    "        'name': 'Union Bordeaux-B√®gles Rugby',\n",
    "        'url': 'https://www.ubbrugby.com/',  # URL principale valide\n",
    "        'type': 'sports'\n",
    "    },\n",
    "    'stade_bordeaux': {\n",
    "        'name': 'Stade de Bordeaux - Matmut Atlantique',\n",
    "        'url': 'https://www.matmut-atlantique.com/',  # URL principale\n",
    "        'type': 'sports'\n",
    "    },\n",
    "    'bordeaux_metropole_sport': {\n",
    "        'name': 'Bordeaux M√©tropole Sports',\n",
    "        'url': 'https://www.bordeaux-metropole.fr/agenda?theme=sport',  # Alternative sportive\n",
    "        'type': 'sports'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sources √©conomiques/business - Sources valid√©es et productives  \n",
    "BUSINESS_SOURCES = {\n",
    "    'bordeaux_business': {\n",
    "        'name': 'Bordeaux Business Events',\n",
    "        'url': 'https://www.bordeaux-metropole.fr/agenda?theme=economie',\n",
    "        'type': 'business'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plateformes sp√©cialis√©es √©v√©nements - Sources valid√©es et productives\n",
    "EVENT_PLATFORMS = {\n",
    "    'univ_bordeaux': {\n",
    "        'name': 'Universit√© de Bordeaux',\n",
    "        'url': 'https://www.u-bordeaux.fr/actualites',\n",
    "        'type': 'education'\n",
    "    },\n",
    "    'bordeaux_metropole_agenda': {\n",
    "        'name': 'Bordeaux M√©tropole - Agenda Complet',\n",
    "        'url': 'https://www.bordeaux-metropole.fr/agenda',\n",
    "        'type': 'platform'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìä Sources optimis√©es par cat√©gorie:\")\n",
    "print(f\"\\nüèõÔ∏è Sources officielles ({len(OFFICIAL_SOURCES)}):\")\n",
    "for key, source in OFFICIAL_SOURCES.items():\n",
    "    print(f\"  - {source['name']}\")\n",
    "\n",
    "print(f\"\\nüé≠ Sources culturelles ({len(CULTURAL_SOURCES)}):\")\n",
    "for key, source in CULTURAL_SOURCES.items():\n",
    "    print(f\"  - {source['name']}\")\n",
    "\n",
    "print(f\"\\n‚öΩ Sources sportives ({len(SPORTS_SOURCES)}):\")\n",
    "for key, source in SPORTS_SOURCES.items():\n",
    "    print(f\"  - {source['name']}\")\n",
    "\n",
    "print(f\"\\nüíº Sources √©conomiques ({len(BUSINESS_SOURCES)}):\")\n",
    "for key, source in BUSINESS_SOURCES.items():\n",
    "    print(f\"  - {source['name']}\")\n",
    "\n",
    "print(f\"\\nüåê Plateformes sp√©cialis√©es ({len(EVENT_PLATFORMS)}):\")\n",
    "for key, source in EVENT_PLATFORMS.items():\n",
    "    print(f\"  - {source['name']}\")\n",
    "\n",
    "total_sources = len(OFFICIAL_SOURCES) + len(CULTURAL_SOURCES) + len(SPORTS_SOURCES) + len(BUSINESS_SOURCES) + len(EVENT_PLATFORMS)\n",
    "print(f\"\\nüìä Total: {total_sources} sources optimis√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d6997",
   "metadata": {},
   "source": [
    "## 4. Cr√©ation des Fonctions de Collecte de Donn√©es\n",
    "\n",
    "D√©veloppement de fonctions r√©utilisables pour scraper diff√©rents types de sites web et extraire les informations d'√©v√©nements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7044902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Fonctions de collecte cr√©√©es avec succ√®s\n",
      "üéØ Mode R√âEL uniquement - Pas de donn√©es d'exemple\n"
     ]
    }
   ],
   "source": [
    "def safe_request(url, timeout=15):\n",
    "    \"\"\"Effectue une requ√™te HTTP s√©curis√©e avec gestion d'erreurs\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None\n",
    "\n",
    "def extract_text_safely(element, selector=None):\n",
    "    \"\"\"Extrait le texte d'un √©l√©ment HTML de mani√®re s√©curis√©e\"\"\"\n",
    "    try:\n",
    "        if selector:\n",
    "            target = element.select_one(selector)\n",
    "            return target.get_text(strip=True) if target else \"\"\n",
    "        return element.get_text(strip=True) if element else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def parse_event_date(date_str):\n",
    "    \"\"\"Parse une cha√Æne de date en format standard\"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Utilise dateparser pour une d√©tection flexible des dates\n",
    "        parsed_date = dateparser.parse(date_str, languages=['fr', 'en'])\n",
    "        if parsed_date:\n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return date_str\n",
    "\n",
    "def is_september_2025(date_str):\n",
    "    \"\"\"V√©rifie si une date correspond √† septembre 2025\"\"\"\n",
    "    try:\n",
    "        if date_str:\n",
    "            parsed_date = dateparser.parse(date_str, languages=['fr', 'en'])\n",
    "            if parsed_date:\n",
    "                return parsed_date.year == 2025 and parsed_date.month == 9\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Accepter tous les √©v√©nements pour l'instant (filtrage plus tard)\n",
    "    return True\n",
    "\n",
    "def create_event_dict(title=\"\", date=\"\", time=\"\", location=\"\", description=\"\", \n",
    "                     category=\"\", price=\"\", url=\"\", source=\"\"):\n",
    "    \"\"\"Cr√©e un dictionnaire standardis√© pour un √©v√©nement\"\"\"\n",
    "    return {\n",
    "        'title': title.strip() if title else \"\",\n",
    "        'date': parse_event_date(date) if date else \"\",\n",
    "        'time': time.strip() if time else \"\",\n",
    "        'location': location.strip() if location else \"\",\n",
    "        'description': description.strip() if description else \"\",\n",
    "        'category': category.strip() if category else \"\",\n",
    "        'price': price.strip() if price else \"\",\n",
    "        'url': url.strip() if url else \"\",\n",
    "        'source': source.strip() if source else \"\",\n",
    "        'scraped_at': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def scrape_with_multiple_selectors(soup, selectors_list):\n",
    "    \"\"\"Essaie plusieurs s√©lecteurs pour trouver des √©v√©nements\"\"\"\n",
    "    events_found = []\n",
    "    \n",
    "    for selector_set in selectors_list:\n",
    "        containers = soup.select(selector_set.get('container', '.event'))\n",
    "        if containers:\n",
    "            for container in containers[:30]:  # Limite raisonnable\n",
    "                try:\n",
    "                    title = extract_text_safely(container, selector_set.get('title'))\n",
    "                    \n",
    "                    # Essayer plusieurs s√©lecteurs pour le titre si n√©cessaire\n",
    "                    if not title:\n",
    "                        for title_sel in ['h1', 'h2', 'h3', 'h4', '.title', '[class*=\"title\"]', '.event-title']:\n",
    "                            title = extract_text_safely(container, title_sel)\n",
    "                            if title and len(title) > 3:\n",
    "                                break\n",
    "                    \n",
    "                    if title and len(title) > 3:\n",
    "                        date = extract_text_safely(container, selector_set.get('date'))\n",
    "                        time = extract_text_safely(container, selector_set.get('time'))\n",
    "                        location = extract_text_safely(container, selector_set.get('location'))\n",
    "                        description = extract_text_safely(container, selector_set.get('description'))\n",
    "                        \n",
    "                        # Essayer d'extraire plus d'informations si disponibles\n",
    "                        if not location:\n",
    "                            for loc_sel in ['.venue', '.lieu', '.location', '[class*=\"lieu\"]', '[class*=\"location\"]']:\n",
    "                                location = extract_text_safely(container, loc_sel)\n",
    "                                if location:\n",
    "                                    break\n",
    "                        \n",
    "                        event = {\n",
    "                            'title': title,\n",
    "                            'date': date,\n",
    "                            'time': time,\n",
    "                            'location': location,\n",
    "                            'description': description\n",
    "                        }\n",
    "                        events_found.append(event)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            if events_found:\n",
    "                break  # Sortir si on a trouv√© des √©v√©nements\n",
    "    \n",
    "    return events_found\n",
    "\n",
    "def scrape_generic_events(url, source_name, selectors):\n",
    "    \"\"\"Fonction de scraping d'√©v√©nements - VERSION R√âELLE UNIQUEMENT\"\"\"\n",
    "    events = []\n",
    "    \n",
    "    response = safe_request(url)\n",
    "    if not response:\n",
    "        return events\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # D√©finir plusieurs strat√©gies de s√©lecteurs\n",
    "    selector_strategies = [\n",
    "        selectors,  # S√©lecteurs sp√©cifiques au site\n",
    "        {\n",
    "            'container': 'article',\n",
    "            'title': 'h1, h2, h3',\n",
    "            'date': '[class*=\"date\"], time',\n",
    "            'location': '[class*=\"lieu\"], [class*=\"location\"]',\n",
    "            'description': 'p, .description'\n",
    "        },\n",
    "        {\n",
    "            'container': '.card, .event, .item',\n",
    "            'title': 'h1, h2, h3, .title',\n",
    "            'date': '.date, time',\n",
    "            'location': '.location, .venue',\n",
    "            'description': 'p'\n",
    "        },\n",
    "        {\n",
    "            'container': '[class*=\"event\"], [class*=\"agenda\"]',\n",
    "            'title': 'h1, h2, h3',\n",
    "            'date': '[class*=\"date\"]',\n",
    "            'location': '[class*=\"lieu\"]',\n",
    "            'description': 'p, div'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    raw_events = scrape_with_multiple_selectors(soup, selector_strategies)\n",
    "    \n",
    "    # Convertir en format standardis√©\n",
    "    for raw_event in raw_events:\n",
    "        if raw_event['title']:  # Au minimum un titre\n",
    "            event = create_event_dict(\n",
    "                title=raw_event['title'],\n",
    "                date=raw_event['date'],\n",
    "                time=raw_event['time'],\n",
    "                location=raw_event['location'],\n",
    "                description=raw_event['description'],\n",
    "                source=source_name,\n",
    "                url=url\n",
    "            )\n",
    "            events.append(event)\n",
    "    \n",
    "    return events\n",
    "\n",
    "print(\"üîß Fonctions de collecte cr√©√©es avec succ√®s\")\n",
    "print(\"üéØ Mode R√âEL uniquement - Pas de donn√©es d'exemple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493973b9",
   "metadata": {},
   "source": [
    "## 5. Scraping des Sources Officielles de Bordeaux\n",
    "\n",
    "Collecte des √©v√©nements depuis les sites officiels de la ville et de la m√©tropole bordelaise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a51eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèõÔ∏è Scraping des sources officielles...\n",
      "‚úÖ 33 √©v√©nements officiels collect√©s\n"
     ]
    }
   ],
   "source": [
    "# S√©lecteurs CSS pour les sources officielles - Optimis√©s pour les sources productives\n",
    "OFFICIAL_SELECTORS = {\n",
    "    'bordeaux_metropole': {\n",
    "        'container': 'a[href*=\"/agenda/\"]',\n",
    "        'title': 'h2, h3, .title',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.location, [class*=\"lieu\"], [class*=\"location\"]',\n",
    "        'description': 'p, .description, .summary'\n",
    "    },\n",
    "    'bordeaux_ville': {\n",
    "        'container': '.event, .agenda-item, article',\n",
    "        'title': 'h1, h2, h3, .title',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.location, [class*=\"lieu\"]',\n",
    "        'description': '.description, .summary, p'\n",
    "    }\n",
    "}\n",
    "\n",
    "official_events = []\n",
    "\n",
    "print(\"üèõÔ∏è Scraping des sources officielles...\")\n",
    "\n",
    "for source_key, source_info in OFFICIAL_SOURCES.items():\n",
    "    selectors = OFFICIAL_SELECTORS.get(source_key, OFFICIAL_SELECTORS['bordeaux_metropole'])\n",
    "    \n",
    "    events = scrape_generic_events(\n",
    "        url=source_info['url'],\n",
    "        source_name=source_info['name'],\n",
    "        selectors=selectors\n",
    "    )\n",
    "    \n",
    "    official_events.extend(events)\n",
    "    random_delay(2, 4)\n",
    "\n",
    "print(f\"‚úÖ {len(official_events)} √©v√©nements officiels collect√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76095144",
   "metadata": {},
   "source": [
    "## 6. Scraping des Plateformes Culturelles\n",
    "\n",
    "Extraction des √©v√©nements culturels depuis les th√©√¢tres, op√©ras, salles de concert et mus√©es bordelais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "505c2587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ Scraping des plateformes culturelles...\n",
      "‚úÖ 42 √©v√©nements culturels collect√©s\n"
     ]
    }
   ],
   "source": [
    "# S√©lecteurs CSS pour les sources culturelles - Am√©lior√©s\n",
    "CULTURAL_SELECTORS = {\n",
    "    'opera_bordeaux': {\n",
    "        'container': '.spectacle, .event, .show, article, [class*=\"event\"]',\n",
    "        'title': '.title, h1, h2, h3, [class*=\"title\"]',\n",
    "        'date': '.date, .dates, time, [class*=\"date\"]',\n",
    "        'location': '.salle, .venue, [class*=\"lieu\"], [class*=\"salle\"]',\n",
    "        'description': '.synopsis, .description, p, .resume'\n",
    "    },\n",
    "    'cap_sciences': {\n",
    "        'container': '.event, .exposition, article, [class*=\"event\"]',\n",
    "        'title': 'h1, h2, h3, .title',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.venue, [class*=\"lieu\"]',\n",
    "        'description': '.description, p, .summary'\n",
    "    },\n",
    "    'bassins_lumieres': {\n",
    "        'container': '.exposition, .event, article',\n",
    "        'title': 'h1, h2, h3, .title',\n",
    "        'date': '.date, .dates, time',\n",
    "        'location': '.lieu, .location',\n",
    "        'description': '.description, p'\n",
    "    },\n",
    "    'musee_mer_marine': {\n",
    "        'container': '.event, .exposition, article',\n",
    "        'title': 'h1, h2, h3, .title',\n",
    "        'date': '.date, time',\n",
    "        'location': '.lieu, .venue',\n",
    "        'description': '.description, p'\n",
    "    }\n",
    "}\n",
    "\n",
    "cultural_events = []\n",
    "\n",
    "print(\"üé≠ Scraping des plateformes culturelles...\")\n",
    "\n",
    "for source_key, source_info in CULTURAL_SOURCES.items():\n",
    "    selectors = CULTURAL_SELECTORS.get(source_key, CULTURAL_SELECTORS['opera_bordeaux'])\n",
    "    \n",
    "    events = scrape_generic_events(\n",
    "        url=source_info['url'],\n",
    "        source_name=source_info['name'],\n",
    "        selectors=selectors\n",
    "    )\n",
    "    \n",
    "    for event in events:\n",
    "        event['category'] = 'Culturel'\n",
    "    \n",
    "    cultural_events.extend(events)\n",
    "    random_delay(2, 4)\n",
    "\n",
    "print(f\"‚úÖ {len(cultural_events)} √©v√©nements culturels collect√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dce0ef",
   "metadata": {},
   "source": [
    "## 7. Scraping des Sources Sportives\n",
    "\n",
    "Collecte des √©v√©nements sportifs depuis les clubs et stades bordelais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5564c2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öΩ Scraping des sources sportives...\n",
      "‚úÖ 37 √©v√©nements sportifs collect√©s\n"
     ]
    }
   ],
   "source": [
    "# S√©lecteurs pour les sources sportives - simplifi√©s\n",
    "SPORTS_SELECTORS = {\n",
    "    'ubb_rugby': {\n",
    "        'container': '.actualite, article, .news, [class*=\"actu\"], [class*=\"news\"]',\n",
    "        'title': 'h1, h2, h3, .title, [class*=\"title\"]',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.lieu, .venue, [class*=\"lieu\"]',\n",
    "        'description': 'p, .description, .summary'\n",
    "    },\n",
    "    'stade_bordeaux': {\n",
    "        'container': '.event, .spectacle, article, [class*=\"event\"]',\n",
    "        'title': 'h1, h2, h3, .title',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.venue, [class*=\"lieu\"]',\n",
    "        'description': 'p, .description'\n",
    "    },\n",
    "    'bordeaux_metropole_sport': {\n",
    "        'container': 'a[href*=\"/agenda/\"]',  # M√™me structure que Bordeaux M√©tropole\n",
    "        'title': 'h2, h3, .title',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.location, [class*=\"lieu\"]',\n",
    "        'description': 'p, .description'\n",
    "    }\n",
    "}\n",
    "\n",
    "def simple_sports_scraping(url, source_name, selectors):\n",
    "    \"\"\"Version simplifi√©e du scraping sportif\"\"\"\n",
    "    events = []\n",
    "    \n",
    "    response = safe_request(url, timeout=10)\n",
    "    if not response:\n",
    "        return events\n",
    "    \n",
    "    events = scrape_generic_events(url, source_name, selectors)\n",
    "    \n",
    "    if len(events) == 0:\n",
    "        alternative_selectors = {\n",
    "            'container': 'article, .card, .item, div[class*=\"event\"], div[class*=\"actu\"]',\n",
    "            'title': 'h1, h2, h3, h4, .title, strong',\n",
    "            'date': '[class*=\"date\"], time, .date',\n",
    "            'location': '[class*=\"lieu\"], [class*=\"location\"], .venue',\n",
    "            'description': 'p, .description, .summary'\n",
    "        }\n",
    "        events = scrape_generic_events(url, source_name, alternative_selectors)\n",
    "    \n",
    "    return events\n",
    "\n",
    "sports_events = []\n",
    "\n",
    "print(\"‚öΩ Scraping des sources sportives...\")\n",
    "\n",
    "for source_key, source_info in SPORTS_SOURCES.items():\n",
    "    selectors = SPORTS_SELECTORS.get(source_key, SPORTS_SELECTORS['ubb_rugby'])\n",
    "    \n",
    "    try:\n",
    "        events = simple_sports_scraping(\n",
    "            url=source_info['url'],\n",
    "            source_name=source_info['name'],\n",
    "            selectors=selectors\n",
    "        )\n",
    "        \n",
    "        for event in events:\n",
    "            event['category'] = 'Sport'\n",
    "        \n",
    "        sports_events.extend(events)\n",
    "        random_delay(1, 2)\n",
    "        \n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ {len(sports_events)} √©v√©nements sportifs collect√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a615ddd",
   "metadata": {},
   "source": [
    "## 8. Scraping des Sources √âconomiques et Business\n",
    "\n",
    "Collecte des √©v√©nements √©conomiques, salons, conf√©rences et √©v√©nements business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65eadaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíº Scraping des sources √©conomiques et business...\n",
      "‚úÖ 20 √©v√©nements √©conomiques collect√©s\n"
     ]
    }
   ],
   "source": [
    "# S√©lecteurs pour les sources √©conomiques - Version simplifi√©e\n",
    "BUSINESS_SELECTORS = {\n",
    "    'bordeaux_business': {\n",
    "        'container': 'a[href*=\"/agenda/\"]',\n",
    "        'title': 'h2, h3, .title',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.location, [class*=\"lieu\"]',\n",
    "        'description': 'p, .description'\n",
    "    }\n",
    "}\n",
    "\n",
    "def enhanced_business_scraping(url, source_name, selectors):\n",
    "    \"\"\"Version simplifi√©e du scraping business\"\"\"\n",
    "    events = []\n",
    "    \n",
    "    response = safe_request(url, timeout=8)\n",
    "    if not response:\n",
    "        return events\n",
    "    \n",
    "    events = scrape_generic_events(url, source_name, selectors)\n",
    "    \n",
    "    if len(events) == 0:\n",
    "        alternative_selectors = {\n",
    "            'container': 'article, .card, .event, .salon, div[class*=\"event\"], div[class*=\"salon\"]',\n",
    "            'title': 'h1, h2, h3, h4, .title, strong, .event-title',\n",
    "            'date': '[class*=\"date\"], time, .date, [datetime]',\n",
    "            'location': '[class*=\"lieu\"], [class*=\"location\"], [class*=\"salle\"], .venue',\n",
    "            'description': 'p, .description, .summary, .abstract'\n",
    "        }\n",
    "        additional_events = scrape_generic_events(url, source_name, alternative_selectors)\n",
    "        events.extend(additional_events)\n",
    "    \n",
    "    return events\n",
    "\n",
    "business_events = []\n",
    "\n",
    "print(\"üíº Scraping des sources √©conomiques et business...\")\n",
    "\n",
    "for source_key, source_info in BUSINESS_SOURCES.items():\n",
    "    selectors = BUSINESS_SELECTORS.get(source_key, BUSINESS_SELECTORS['bordeaux_business'])\n",
    "    \n",
    "    try:\n",
    "        events = enhanced_business_scraping(\n",
    "            url=source_info['url'],\n",
    "            source_name=source_info['name'],\n",
    "            selectors=selectors\n",
    "        )\n",
    "        \n",
    "        for event in events:\n",
    "            event['category'] = '√âconomique'\n",
    "        \n",
    "        business_events.extend(events)\n",
    "        random_delay(1, 2)\n",
    "        \n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ {len(business_events)} √©v√©nements √©conomiques collect√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12e6fb",
   "metadata": {},
   "source": [
    "## 9. Scraping des Plateformes Sp√©cialis√©es\n",
    "\n",
    "Collecte depuis les plateformes universitaires et sites sp√©cialis√©s dans les √©v√©nements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcf32356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Scraping des plateformes sp√©cialis√©es...\n",
      "‚úÖ 21 √©v√©nements de plateformes collect√©s\n"
     ]
    }
   ],
   "source": [
    "# S√©lecteurs pour les plateformes sp√©cialis√©es - Version optimis√©e\n",
    "PLATFORM_SELECTORS = {\n",
    "    'univ_bordeaux': {\n",
    "        'container': '.actualite, .news, article, [class*=\"actu\"], [class*=\"news\"]',\n",
    "        'title': 'h1, h2, h3, .title, [class*=\"title\"]',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.lieu, .venue, [class*=\"lieu\"]',\n",
    "        'description': 'p, .description, .summary'\n",
    "    },\n",
    "    'bordeaux_metropole_agenda': {\n",
    "        'container': 'a[href*=\"/agenda/\"]',\n",
    "        'title': 'h2, h3, .title',\n",
    "        'date': '.date, time, [class*=\"date\"]',\n",
    "        'location': '.location, [class*=\"lieu\"]',\n",
    "        'description': 'p, .description'\n",
    "    }\n",
    "}\n",
    "\n",
    "platform_events = []\n",
    "\n",
    "print(\"üåê Scraping des plateformes sp√©cialis√©es...\")\n",
    "\n",
    "for source_key, source_info in EVENT_PLATFORMS.items():\n",
    "    selectors = PLATFORM_SELECTORS.get(source_key, PLATFORM_SELECTORS['univ_bordeaux'])\n",
    "    \n",
    "    try:\n",
    "        events = scrape_generic_events(\n",
    "            url=source_info['url'],\n",
    "            source_name=source_info['name'],\n",
    "            selectors=selectors\n",
    "        )\n",
    "        \n",
    "        for event in events:\n",
    "            if 'univ' in source_key.lower():\n",
    "                event['category'] = '√âducation'\n",
    "            else:\n",
    "                event['category'] = 'G√©n√©ral'\n",
    "        \n",
    "        platform_events.extend(events)\n",
    "        random_delay(1, 2)\n",
    "        \n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "print(f\"‚úÖ {len(platform_events)} √©v√©nements de plateformes collect√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c87cf",
   "metadata": {},
   "source": [
    "## 10. Sauvegarde des Donn√©es Collect√©es\n",
    "\n",
    "Export des donn√©es brutes collect√©es vers les formats CSV et JSON pour traitement ult√©rieur dans un notebook d√©di√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad4a9f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Collecte termin√©e - Pr√©paration de la sauvegarde...\n",
      "\n",
      "üìä RAPPORT DE COLLECTE PAR CAT√âGORIE:\n",
      "  üèõÔ∏è Sources officielles: 33 √©v√©nements\n",
      "  üé≠ Sources culturelles: 42 √©v√©nements\n",
      "  ‚öΩ Sources sportives: 37 √©v√©nements\n",
      "  üíº Sources √©conomiques: 20 √©v√©nements\n",
      "  üåê Plateformes sp√©cialis√©es: 21 √©v√©nements\n",
      "  üìà TOTAL COLLECT√â: 153 √©v√©nements\n",
      "\n",
      "‚úÖ Collecte r√©ussie - 153 √©v√©nements pr√™ts pour sauvegarde\n",
      "\n",
      "üíæ Sauvegarde des donn√©es brutes collect√©es...\n",
      "  ‚úÖ official: 33 √©v√©nements ‚Üí bordeaux_events_official_20250829_171417.json\n",
      "  ‚úÖ cultural: 42 √©v√©nements ‚Üí bordeaux_events_cultural_20250829_171417.json\n",
      "  ‚úÖ sports: 37 √©v√©nements ‚Üí bordeaux_events_sports_20250829_171417.json\n",
      "  ‚úÖ business: 20 √©v√©nements ‚Üí bordeaux_events_business_20250829_171417.json\n",
      "  ‚úÖ platforms: 21 √©v√©nements ‚Üí bordeaux_events_platforms_20250829_171417.json\n",
      "\n",
      "üíæ Donn√©es consolid√©es: bordeaux_events_all_raw_20250829_171417.json\n",
      "üìã M√©tadonn√©es: metadata_collection_20250829_171417.json\n",
      "\n",
      "üéâ COLLECTE TERMIN√âE AVEC SUCC√àS!\n",
      "üìä R√âSUM√â:\n",
      "  üìÅ Dossier de sauvegarde: ../data/raw\n",
      "  üéØ Total √©v√©nements collect√©s: 153\n",
      "  üì¶ Fichiers JSON g√©n√©r√©s: 7\n"
     ]
    }
   ],
   "source": [
    "# Consolidation de tous les √©v√©nements collect√©s par cat√©gorie\n",
    "print(\"üíæ Collecte termin√©e - Pr√©paration de la sauvegarde...\\n\")\n",
    "\n",
    "# Consolidation avec TOUS les √©v√©nements scrap√©s par cat√©gorie\n",
    "all_events = official_events + cultural_events + sports_events + business_events + platform_events\n",
    "\n",
    "print(\"üìä RAPPORT DE COLLECTE PAR CAT√âGORIE:\")\n",
    "print(f\"  üèõÔ∏è Sources officielles: {len(official_events)} √©v√©nements\")\n",
    "print(f\"  üé≠ Sources culturelles: {len(cultural_events)} √©v√©nements\")\n",
    "print(f\"  ‚öΩ Sources sportives: {len(sports_events)} √©v√©nements\")\n",
    "print(f\"  üíº Sources √©conomiques: {len(business_events)} √©v√©nements\")\n",
    "print(f\"  üåê Plateformes sp√©cialis√©es: {len(platform_events)} √©v√©nements\")\n",
    "print(f\"  üìà TOTAL COLLECT√â: {len(all_events)} √©v√©nements\")\n",
    "\n",
    "if len(all_events) > 0:\n",
    "    print(f\"\\n‚úÖ Collecte r√©ussie - {len(all_events)} √©v√©nements pr√™ts pour sauvegarde\")\n",
    "else:\n",
    "    print(\"\\n‚ùå AUCUN √âV√âNEMENT COLLECT√â!\")\n",
    "    print(\"‚ö†Ô∏è V√©rifier la connectivit√© et les URLs des sources\")\n",
    "\n",
    "# Cr√©ation des dossiers de sauvegarde\n",
    "data_dir = Path('../data')\n",
    "raw_dir = data_dir / 'raw'\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# G√©n√©ration du timestamp pour les fichiers\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"\\nüíæ Sauvegarde des donn√©es brutes collect√©es...\")\n",
    "\n",
    "# Sauvegarde des donn√©es brutes par cat√©gorie (JSON uniquement)\n",
    "categories_data = {\n",
    "    'official': official_events,\n",
    "    'cultural': cultural_events,\n",
    "    'sports': sports_events,\n",
    "    'business': business_events,\n",
    "    'platforms': platform_events\n",
    "}\n",
    "\n",
    "# Sauvegarde par cat√©gorie en JSON\n",
    "for category, events in categories_data.items():\n",
    "    if events:\n",
    "        category_json = raw_dir / f'bordeaux_events_{category}_{timestamp}.json'\n",
    "        with open(category_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(events, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"  ‚úÖ {category}: {len(events)} √©v√©nements ‚Üí {category_json.name}\")\n",
    "\n",
    "# Sauvegarde des donn√©es consolid√©es brutes (JSON uniquement)\n",
    "if all_events:\n",
    "    raw_json_path = raw_dir / f'bordeaux_events_all_raw_{timestamp}.json'\n",
    "    with open(raw_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_events, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nüíæ Donn√©es consolid√©es: {raw_json_path.name}\")\n",
    "\n",
    "# Sauvegarde des m√©tadonn√©es de collecte\n",
    "metadata = {\n",
    "    'collection_info': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'target_month': 'septembre 2025',\n",
    "        'total_sources_configured': sum([len(OFFICIAL_SOURCES), len(CULTURAL_SOURCES), \n",
    "                                        len(SPORTS_SOURCES), len(BUSINESS_SOURCES), \n",
    "                                        len(EVENT_PLATFORMS)])\n",
    "    },\n",
    "    'collection_results': {\n",
    "        'total_events_collected': len(all_events),\n",
    "        'events_by_category': {\n",
    "            'official': len(official_events),\n",
    "            'cultural': len(cultural_events),\n",
    "            'sports': len(sports_events),\n",
    "            'business': len(business_events),\n",
    "            'platforms': len(platform_events)\n",
    "        }\n",
    "    },\n",
    "    'sources_used': {\n",
    "        'official_sources': [source['name'] for source in OFFICIAL_SOURCES.values()],\n",
    "        'cultural_sources': [source['name'] for source in CULTURAL_SOURCES.values()],\n",
    "        'sports_sources': [source['name'] for source in SPORTS_SOURCES.values()],\n",
    "        'business_sources': [source['name'] for source in BUSINESS_SOURCES.values()],\n",
    "        'platform_sources': [source['name'] for source in EVENT_PLATFORMS.values()]\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = raw_dir / f'metadata_collection_{timestamp}.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"üìã M√©tadonn√©es: {metadata_path.name}\")\n",
    "\n",
    "# Rapport final de collecte\n",
    "if all_events:\n",
    "    print(f\"\\nüéâ COLLECTE TERMIN√âE AVEC SUCC√àS!\")\n",
    "    print(f\"üìä R√âSUM√â:\")\n",
    "    print(f\"  üìÅ Dossier de sauvegarde: {raw_dir}\")\n",
    "    print(f\"  üéØ Total √©v√©nements collect√©s: {len(all_events)}\")\n",
    "    print(f\"  üì¶ Fichiers JSON g√©n√©r√©s: {len(categories_data) + 2}\")  # cat√©gories + consolid√© + m√©tadonn√©es\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Aucune donn√©e collect√©e √† sauvegarder\")\n",
    "    print(f\"V√©rifier la connectivit√© et les sources de donn√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3574f",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Ce notebook a collect√© avec succ√®s des donn√©es d'√©v√©nements √† Bordeaux depuis 12 sources diff√©rentes r√©parties en 5 cat√©gories. Les donn√©es brutes ont √©t√© sauvegard√©es au format JSON pour faciliter le traitement ult√©rieur.\n",
    "\n",
    "**Sources les plus productives :**\n",
    "- Bordeaux M√©tropole et Ville de Bordeaux (sources officielles)\n",
    "- Op√©ra National de Bordeaux et mus√©es (sources culturelles)\n",
    "- UBB Rugby et Stade Matmut Atlantique (sources sportives)\n",
    "\n",
    "**R√©sultats :** Collection automatis√©e d'√©v√©nements bordelais avec cat√©gorisation par type de source et sauvegarde structur√©e des donn√©es brutes.\n",
    "\n",
    "---\n",
    "\n",
    "*Bordeaux Events Analytics - Phase 1: Collection des donn√©es*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
